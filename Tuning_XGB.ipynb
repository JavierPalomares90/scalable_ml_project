{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils as utils\n",
    "import models.xgboost_model as xgb_model\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_data():\n",
    "    # Read csv files of saved pitch data from the MLB 2016-2019 seasons\n",
    "    pitch_data = pd.read_csv('raw_pitch_data_all_base_v2.csv', index_col=0)\n",
    "    print(\"pitch data loaded\")\n",
    "    return pitch_data\n",
    "\n",
    "def filter_pitch_data(pitch_data):\n",
    "    pre_filter_rows = len(pitch_data.index)\n",
    "    pitch_data = pitch_data[pd.notnull(pitch_data['p1_pitch_type'])]\n",
    "    post_filter_rows = len(pitch_data.index)\n",
    "\n",
    "    filter_diff = pre_filter_rows - post_filter_rows\n",
    "    filter_pcnt = (filter_diff)/pre_filter_rows\n",
    "\n",
    "    print('Removed Null/NaN labeled pitch types rows, filtered %d of %d rows at %f%%' % (filter_diff, pre_filter_rows, filter_pcnt))\n",
    "    return pitch_data\n",
    "\n",
    "def drop_pitch_types(pitch_data):\n",
    "    #\n",
    "    # Drop rows with unwanted pitchtypes (including automatic ball/strikes, pitchouts, etc)\n",
    "    #\n",
    "    pre_filter_rows = len(pitch_data.index)\n",
    "    pitch_data = utils.drop_unwanted_pitches(pitch_data)\n",
    "    post_filter_rows = len(pitch_data.index)\n",
    "\n",
    "    filter_diff = pre_filter_rows - post_filter_rows\n",
    "    filter_pcnt = (filter_diff)/pre_filter_rows\n",
    "\n",
    "    print('Removed rows w/ unwanted pitch types, filtered %d of %d rows at %f%%' % (filter_diff, pre_filter_rows, filter_pcnt))\n",
    "    return pitch_data\n",
    "\n",
    "def drop_columns(pitch_data):\n",
    "    #\n",
    "    # Drop unwanted dataset columns \n",
    "    # \n",
    "\n",
    "    # ID columns to drop\n",
    "    id_cols_to_drop=['p1_pitch_id','p0_pitch_id','pitch_data_id','team_id','game_id',\n",
    "                    'inning_id','half_inning_id','at_bat_id','gid','b1_id','b1_team_id',\n",
    "                    'team_abbrev']\n",
    "    pitch_data = utils.drop_columns_by_list(pitch_data,id_cols_to_drop)\n",
    "    # Pitch data columns to drop\n",
    "    pitch_cols_to_drop = ['p0_pitch_seqno', 'p1_pitch_seqno', 'p0_inning', 'result_type',\n",
    "                          'type_confidence', 'p0_at_bat_o', 'p0_pitch_des', 'nasty']\n",
    "    pitch_data = utils.drop_columns_by_list(pitch_data, pitch_cols_to_drop)\n",
    "\n",
    "    # Optional pitchf/x data columns to drop\n",
    "    #pitchfx_cols_to_drop = ['pitch_count_atbat', 'pitch_count_team', 'start_speed', 'spin_dir',\n",
    "    #                        'x', 'y', 'sz_top', 'sz_bot', 'pfx_x', 'pfx_z', 'px', 'pz',\n",
    "    #                        'x0', 'y0', 'z0', 'vx0', 'vy0', 'vz0', 'ax', 'ay', 'az', 'break_y']\n",
    "    #pitch_data = utils.drop_columns_by_list(pitch_data, pitchfx_cols_to_drop)\n",
    "\n",
    "    print(\"dropped cols\")\n",
    "    return pitch_data\n",
    "\n",
    "def add_run_diff(pitch_data):\n",
    "    #\n",
    "    # Create new column of run differential\n",
    "    #\n",
    "    pitch_data['run_diff'] = pitch_data['runs_pitcher_team'] - pitch_data['runs_batter_team']\n",
    "    cols_to_drop=['runs_pitcher_team','runs_batter_team']\n",
    "    pitch_data = utils.drop_columns_by_list(pitch_data, cols_to_drop)\n",
    "    print(\"added run diff\")\n",
    "    return pitch_data\n",
    "\n",
    "def add_crunch_time(pitch_data):\n",
    "    #\n",
    "    # Create new column for crunch time (after 7th inning)\n",
    "    #\n",
    "    pitch_data['inning'] = pitch_data['inning'].astype(dtype='int64')\n",
    "    pitch_data['inning'] = pitch_data['inning'].fillna(0)  # '0' is for unknown inning (Other values are 1-9)\n",
    "    pitch_data['crunch_time'] = np.where(pitch_data['inning'] > 7, 1, 0)\n",
    "    cols_to_drop=['inning']\n",
    "    pitch_data = utils.drop_columns_by_list(pitch_data, cols_to_drop)\n",
    "    print(\"added crunch time\")\n",
    "    return pitch_data\n",
    "\n",
    "def replace_nans(pitch_data):\n",
    "    #\n",
    "    # Replace Nulls/NaN values that are left in the remaining object columns\n",
    "    #\n",
    "    #\n",
    "    # Replace Nulls/NaN values that are left in the remaining object columns\n",
    "    #\n",
    "    pitch_data['p0_pitch_type'] = pitch_data['p0_pitch_type'].fillna('NP')  # 'NP' is for No Pitch\n",
    "\n",
    "    pitch_data['result_type_simple'] = pitch_data['result_type_simple'].fillna('X')  # 'X' is for in play\n",
    "\n",
    "    pitch_data['b1_game_position'] = pitch_data['b1_game_position'].fillna('Unknown')\n",
    "\n",
    "    pitch_data['b1_bats'] = pitch_data['b1_bats'].fillna('R')  # 'R' is for right handed (Other values are L or S)\n",
    "\n",
    "    pitch_data['throws'] = pitch_data['throws'].fillna('R')  # 'R' is for right handed (Other value is L)\n",
    "\n",
    "    #pitch_data['inning'] = pitch_data['inning'].fillna('0')  # '0' is for unknown inning (Other values are 1-9)\n",
    "\n",
    "    print('Current number of dataframe Null/NaN values: %d' % (pitch_data.isnull().sum().sum()))\n",
    "    #\n",
    "    # Fill the rest of Null/NaN values with zero in numeric columns\n",
    "    #\n",
    "    replace_dict = {'nasty': 0, 'x': 0, 'y': 0, 'sz_top': 0, 'sz_bot': 0, 'pfx_x': 0, 'pfx_z': 0,\n",
    "                    'px': 0, 'pz': 0, 'x0': 0, 'y0': 0, 'z0': 0, 'vx0': 0, 'vy0': 0, 'vz0': 0,\n",
    "                    'ax': 0, 'ay': 0, 'az': 0, 'break_y': 0, 'break_angle': 0, 'break_length': 0,\n",
    "                    'start_speed': 0, 'end_speed': 0, 'zone': 0, 'outcome': 0, 'spin_rate': 0,\n",
    "                    'spin_dir': 0, 'pitch_count_at_bat': 0, 'pitch_count_team': 0,\n",
    "                    'wins': 0, 'losses': 0, 'b1_bat_order': 0}\n",
    "    pitch_data = pitch_data.fillna(value=replace_dict)\n",
    "\n",
    "    print('Current number of dataframe Null/NaN values: %d' % (pitch_data.isnull().sum().sum()))\n",
    "\n",
    "    return pitch_data\n",
    "\n",
    "def encode_object_data(pitch_data):\n",
    "    print('Encoding pitch dataframe of shape {}...'.format(pitch_data.shape))\n",
    "\n",
    "    # Split label column from rest of pitch dataframe then encode\n",
    "    Y_all = pitch_data.loc[:, 'p1_pitch_type'].copy()\n",
    "    Y_all = utils.encode_simple_pitch_types(Y_all)\n",
    "\n",
    "    # Drop label colum from pitch dataframe, then one-hot-encode object columns\n",
    "    pitch_data = pitch_data.drop('p1_pitch_type', axis=1)\n",
    "    pitch_data = utils.one_hot_encode(pitch_data,False)\n",
    "\n",
    "    # Insert label data back into pitch dataframe\n",
    "    pitch_data['p1_pitch_type'] = Y_all.copy()\n",
    "\n",
    "    print('Pitch dataframe encoding complete. New shape: {}'.format(pitch_data.shape))\n",
    "    return pitch_data\n",
    "\n",
    "def split_train_test(pitch_data):\n",
    "    pd_train = pitch_data[pitch_data['season']!=2019].copy()\n",
    "    pd_test = pitch_data[pitch_data['season']==2019].copy()\n",
    "\n",
    "    print('Shape of ALL training data set is {}'.format(pd_train.shape))\n",
    "    print('Shape of ALL test data set is {}'.format(pd_test.shape))\n",
    "\n",
    "    return pd_train,pd_test\n",
    "\n",
    "def get_pitcher_data(pd_train,pd_test,pitcher_id):\n",
    "    pd_train_pitcher = pd_train[pd_train['pitcher_id']==pitcher_id].copy()\n",
    "    pd_test_pitcher = pd_test[pd_test['pitcher_id']==pitcher_id].copy()\n",
    "    return pd_train_pitcher,pd_test_pitcher\n",
    "\n",
    "\n",
    "def drop_season_pitch_id_cols(pd_train,pd_test):\n",
    "    cols_to_drop=['season','pitcher_id']\n",
    "    pd_test = utils.drop_columns_by_list(pd_test, cols_to_drop)\n",
    "    pd_train = utils.drop_columns_by_list(pd_train, cols_to_drop)\n",
    "    return pd_train,pd_test\n",
    "\n",
    "def get_X_Y(pitch_data,num_pitch_types):\n",
    "    X = pitch_data.drop('p1_pitch_type',axis=1).copy()\n",
    "    Y = pitch_data.loc[:,'p1_pitch_type'].copy()\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Get the data and do all necessary feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitch data loaded\n",
      "Removed Null/NaN labeled pitch types rows, filtered 33043 of 2906621 rows at 0.011368%\n",
      "Removed rows w/ unwanted pitch types, filtered 5700 of 2873578 rows at 0.001984%\n",
      "dropped cols\n",
      "added run diff\n",
      "added crunch time\n",
      "Current number of dataframe Null/NaN values: 4550085\n",
      "Current number of dataframe Null/NaN values: 0\n",
      "Encoding pitch dataframe of shape (2867878, 53)...\n",
      "Pitch dataframe encoding complete. New shape: (2867878, 83)\n",
      "Shape of ALL training data set is (2125821, 83)\n",
      "Shape of ALL test data set is (742057, 83)\n",
      "Verlander pitch data rows: train=10613, test=3256.\n",
      "Scherzer pitch data rows: train=10105, test=2657.\n",
      "Porcello pitch data rows: train=9762, test=2960.\n"
     ]
    }
   ],
   "source": [
    "pitch_data = get_pitch_data()\n",
    "pitch_data = filter_pitch_data(pitch_data)\n",
    "pitch_data = drop_pitch_types(pitch_data)\n",
    "pitch_data = drop_columns(pitch_data)\n",
    "pitch_data = add_run_diff(pitch_data)\n",
    "pitch_data = add_crunch_time(pitch_data)\n",
    "# Set intended data types of the remaining columns\n",
    "pitch_data = utils.set_dtypes(pitch_data)\n",
    "pitch_data['season'] = pitch_data['season'].astype(dtype='int64')\n",
    "pitch_data['pitcher_id'] = pitch_data['pitcher_id'].astype(dtype='int64')\n",
    "pitch_data = replace_nans(pitch_data)\n",
    "pitch_data = encode_object_data(pitch_data)\n",
    "pd_train,pd_test = split_train_test(pitch_data)\n",
    "# get the data for top 3 pitchers\n",
    "pd_train_verlander,pd_test_verlander = get_pitcher_data(pd_train,pd_test,434378)\n",
    "pd_train_scherzer,pd_test_scherzer= get_pitcher_data(pd_train,pd_test,453286)\n",
    "pd_train_porcello,pd_test_porcello= get_pitcher_data(pd_train,pd_test,519144)\n",
    "print('Verlander pitch data rows: train=%d, test=%d.' % (len(pd_train_verlander.index), len(pd_test_verlander.index)))\n",
    "print('Scherzer pitch data rows: train=%d, test=%d.' % (len(pd_train_scherzer.index), len(pd_test_scherzer.index)))\n",
    "print('Porcello pitch data rows: train=%d, test=%d.' % (len(pd_train_porcello.index), len(pd_test_porcello.index)))\n",
    "\n",
    "# Lastly drop season and pitch_id columns\n",
    "pd_train,pd_test = drop_season_pitch_id_cols(pd_train,pd_test)\n",
    "pd_train_verlander,pd_test_verlander = drop_season_pitch_id_cols(pd_train_verlander,pd_test_verlander)\n",
    "pd_train_scherzer,pd_test_scherzer = drop_season_pitch_id_cols(pd_train_scherzer,pd_test_scherzer)\n",
    "pd_train_porcello,pd_test_porcello = drop_season_pitch_id_cols(pd_train_porcello,pd_test_porcello)\n",
    "\n",
    "num_pitch_types = 16\n",
    "# get the NN data for Verlander\n",
    "X_test_verlander,Y_test_verlander = get_X_Y(pd_test_verlander,num_pitch_types)\n",
    "X_train_verlander,Y_train_verlander = get_X_Y(pd_train_verlander,num_pitch_types)\n",
    "num_cols = len(X_test_verlander.iloc[0,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get class weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "pitch_types = list(np.unique(Y_train_verlander))\n",
    "class_weights = list(class_weight.compute_class_weight('balanced',\n",
    "                                             pitch_types,\n",
    "                                             Y_train_verlander))\n",
    "\n",
    "w_array = np.ones(Y_train_verlander.shape[0], dtype = 'float')\n",
    "for index, pitch_type in enumerate(Y_train_verlander):\n",
    "    i = pitch_types.index(pitch_type)\n",
    "    w_array[index] = class_weights[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Train an XGBoost Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model the model for verlander\n",
    "model_verlander = xgb_model.get_multi_class_classifier_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Fix learning rate and number of estimators for tuning tree-based parameters.\n",
    "In order to decide on boosting parameters, we need to set some initial values of other parameters. Lets take the following values:\n",
    "\n",
    "* max_depth = 5 : This should be between 3-10. I\u0019ve started with 5 but you can choose a different number as well. 4-6 can be good starting points.\n",
    "* min_child_weight = 1 : A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.\n",
    "* gamma = 0 : A smaller value like 0.1-0.2 can also be chosen for starting. This will anyways be tuned later.\n",
    "* subsample, colsample_bytree = 0.8 : This is a commonly used used start value. Typical values range between 0.5-0.9.\n",
    "* scale_pos_weight- I'll ignore it for now, but if we need to, we'll adjust the weigths by the occurence of each class\n",
    "\n",
    "Please note that all the above are just initial estimates and will be tuned later. Lets take the default learning rate of 0.1 here and check the optimum number of trees using cv function of xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy (Train) : 0.7000848016583435\n",
      "Accuracy (Test) : 0.5276412776412777\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit_multi_class_model(model=model_verlander,x_train=X_train_verlander,y_train=Y_train_verlander,x_test=X_test_verlander,y_test=Y_test_verlander,save_location='verlander.bin',useTrainCV=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimum number of estimators is 109\n"
     ]
    }
   ],
   "source": [
    "n_estimators= model_verlander.n_estimators\n",
    "print(\"The optimum number of estimators is {}\".format(n_estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Tune max_depth and min_child_weight\n",
    "We tune these first as they will have the highest impact on model outcome. To start with, let\u0019s set wider ranges and then we will perform another iteration for smaller ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([12.15200033, 29.09850039, 19.46975861, 11.35141468, 18.35585723,\n",
       "         20.09728403, 36.42628832, 39.03776622, 29.13407955, 32.86936731]),\n",
       "  'std_fit_time': array([1.23697548, 2.80626469, 1.48634652, 1.36350477, 1.54333778,\n",
       "         0.86944595, 2.82808895, 1.77809605, 1.96830722, 4.70982964]),\n",
       "  'mean_score_time': array([0.03584948, 0.06521745, 0.05176139, 0.03277383, 0.04855232,\n",
       "         0.05988235, 0.07033405, 0.09238815, 0.07239819, 0.06583562]),\n",
       "  'std_score_time': array([0.00606894, 0.00839752, 0.00816775, 0.00333232, 0.00398273,\n",
       "         0.01022954, 0.00514603, 0.00512027, 0.00761818, 0.01432484]),\n",
       "  'param_min_child_weight': masked_array(data=[1, 1, 3, 5, 5, 1, 5, 1, 3, 3],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_max_depth': masked_array(data=[3, 7, 5, 3, 5, 5, 9, 9, 7, 9],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'min_child_weight': 1, 'max_depth': 3},\n",
       "   {'min_child_weight': 1, 'max_depth': 7},\n",
       "   {'min_child_weight': 3, 'max_depth': 5},\n",
       "   {'min_child_weight': 5, 'max_depth': 3},\n",
       "   {'min_child_weight': 5, 'max_depth': 5},\n",
       "   {'min_child_weight': 1, 'max_depth': 5},\n",
       "   {'min_child_weight': 5, 'max_depth': 9},\n",
       "   {'min_child_weight': 1, 'max_depth': 9},\n",
       "   {'min_child_weight': 3, 'max_depth': 7},\n",
       "   {'min_child_weight': 3, 'max_depth': 9}],\n",
       "  'split0_test_score': array([-1.28117392, -1.52574509, -1.45911205, -1.27850119, -1.46624026,\n",
       "         -1.49359256, -1.55726218, -1.58475883, -1.53979522, -1.58152408]),\n",
       "  'split1_test_score': array([-1.62751404, -1.78537682, -1.78826191, -1.67275958, -1.76682884,\n",
       "         -1.74008633, -1.84282671, -1.80897026, -1.78278747, -1.82375742]),\n",
       "  'split2_test_score': array([-1.46582957, -1.67647774, -1.59043665, -1.47777171, -1.62509452,\n",
       "         -1.59161982, -1.68933484, -1.68804684, -1.68800232, -1.69014862]),\n",
       "  'split3_test_score': array([-1.41330613, -1.43492743, -1.4432454 , -1.41652921, -1.44729134,\n",
       "         -1.47013314, -1.46750619, -1.48198127, -1.46170746, -1.47678084]),\n",
       "  'split4_test_score': array([-1.00774239, -1.04811338, -1.0204156 , -1.00313483, -1.01685123,\n",
       "         -1.03085845, -1.05810939, -1.07667118, -1.03954817, -1.06707467]),\n",
       "  'mean_test_score': array([-1.35911321, -1.49412809, -1.46029432, -1.3697393 , -1.46446124,\n",
       "         -1.46525806, -1.52300786, -1.52808568, -1.50236813, -1.52785713]),\n",
       "  'std_test_score': array([0.2077981 , 0.25357866, 0.25230343, 0.22289227, 0.25214357,\n",
       "         0.23707699, 0.26470102, 0.25043811, 0.25704142, 0.25756743]),\n",
       "  'rank_test_score': array([ 1,  6,  3,  2,  4,  5,  8, 10,  7,  9], dtype=int32)},\n",
       " {'min_child_weight': 1, 'max_depth': 3},\n",
       " -1.3591132107282138)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "gsearch1 = RandomizedSearchCV(\n",
    "    estimator = xgb_model.get_multi_class_classifier_model(n_estimators=109),\n",
    "    param_distributions = param_test1,\n",
    "    scoring='neg_log_loss',\n",
    "    n_jobs=4,iid=False, \n",
    "    cv=5)\n",
    "gsearch1.fit(X_train_verlander,Y_train_verlander)\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal values for max_depth is 3, and min_child_weight is 1. Lets go one step deeper and look for optimum values. We'll search for values 1 above and below the optimum values because we took an interval of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[0,1,2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([ 8.87722254,  8.32450824,  8.76125493, 13.15802231, 12.79931173,\n",
       "         14.01261115, 16.94193692, 16.86922336, 13.83197494]),\n",
       "  'std_fit_time': array([1.08816791, 0.80801095, 1.42527803, 0.89520265, 1.10634624,\n",
       "         0.92069996, 1.27101805, 2.22583054, 3.7054102 ]),\n",
       "  'mean_score_time': array([0.02944527, 0.03057122, 0.03178706, 0.04326315, 0.02985783,\n",
       "         0.0338356 , 0.05261626, 0.05556507, 0.0364553 ]),\n",
       "  'std_score_time': array([0.00834522, 0.00688399, 0.00442575, 0.0083949 , 0.00570628,\n",
       "         0.00276892, 0.01334068, 0.01599322, 0.00730636]),\n",
       "  'param_min_child_weight': masked_array(data=[0, 1, 2, 0, 1, 2, 0, 1, 2],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_max_depth': masked_array(data=[2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'min_child_weight': 0, 'max_depth': 2},\n",
       "   {'min_child_weight': 1, 'max_depth': 2},\n",
       "   {'min_child_weight': 2, 'max_depth': 2},\n",
       "   {'min_child_weight': 0, 'max_depth': 3},\n",
       "   {'min_child_weight': 1, 'max_depth': 3},\n",
       "   {'min_child_weight': 2, 'max_depth': 3},\n",
       "   {'min_child_weight': 0, 'max_depth': 4},\n",
       "   {'min_child_weight': 1, 'max_depth': 4},\n",
       "   {'min_child_weight': 2, 'max_depth': 4}],\n",
       "  'split0_test_score': array([-1.13456174, -1.13341579, -1.1313469 , -1.26783854, -1.28117392,\n",
       "         -1.28953497, -1.36157448, -1.3614682 , -1.36226322]),\n",
       "  'split1_test_score': array([-1.41632709, -1.41510744, -1.40232305, -1.64232677, -1.62751404,\n",
       "         -1.66484515, -1.75430507, -1.73897851, -1.75892162]),\n",
       "  'split2_test_score': array([-1.31042891, -1.30817036, -1.31092214, -1.47430444, -1.46582957,\n",
       "         -1.465762  , -1.55903383, -1.56642736, -1.56336802]),\n",
       "  'split3_test_score': array([-1.2603335 , -1.27424982, -1.28113625, -1.4052312 , -1.41330613,\n",
       "         -1.45363297, -1.43915145, -1.45867763, -1.46449583]),\n",
       "  'split4_test_score': array([-0.99374876, -0.99406296, -0.99254452, -1.01132331, -1.00774239,\n",
       "         -1.00336201, -1.01531404, -1.00884157, -1.00935733]),\n",
       "  'mean_test_score': array([-1.22308   , -1.22500127, -1.22365457, -1.36020485, -1.35911321,\n",
       "         -1.37542742, -1.42587578, -1.42687865, -1.4316812 ]),\n",
       "  'std_test_score': array([0.14614403, 0.14642628, 0.14475825, 0.21207957, 0.2077981 ,\n",
       "         0.22083897, 0.24432302, 0.24369032, 0.24846763]),\n",
       "  'rank_test_score': array([1, 3, 2, 5, 4, 6, 7, 8, 9], dtype=int32)},\n",
       " {'min_child_weight': 0, 'max_depth': 2},\n",
       " -1.2230800008687082)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "gsearch2 = RandomizedSearchCV(\n",
    "    estimator = xgb_model.get_multi_class_classifier_model(n_estimators=109),\n",
    "    param_distributions = param_test2,\n",
    "    scoring='neg_log_loss',\n",
    "    n_jobs=4,iid=False, \n",
    "    cv=5)\n",
    "gsearch2.fit(X_train_verlander,Y_train_verlander)\n",
    "gsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I see the optimal max_depth=2, and min_child_weight=0\n",
    "#### Step 3: Tune gamma\n",
    "Now lets tune gamma value using the parameters already tuned above. Gamma can take various values but I\u0019ll check for 5 values here. You can go into more precise values as.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([10.15342522,  9.77532272,  9.38416305,  8.65107126,  6.69193392]),\n",
       "  'std_fit_time': array([0.54686812, 0.74029761, 0.5289003 , 0.61512142, 1.64500664]),\n",
       "  'mean_score_time': array([0.02648191, 0.03031607, 0.03076482, 0.02716904, 0.0219852 ]),\n",
       "  'std_score_time': array([0.0054217 , 0.00331199, 0.00847678, 0.00374038, 0.00438532]),\n",
       "  'param_gamma': masked_array(data=[0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "               mask=[False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'gamma': 0.0},\n",
       "   {'gamma': 0.1},\n",
       "   {'gamma': 0.2},\n",
       "   {'gamma': 0.3},\n",
       "   {'gamma': 0.4}],\n",
       "  'split0_test_score': array([-1.13456174, -1.13462285, -1.13470922, -1.13474075, -1.1343911 ]),\n",
       "  'split1_test_score': array([-1.41632709, -1.41664587, -1.41689593, -1.41639575, -1.40161711]),\n",
       "  'split2_test_score': array([-1.31042891, -1.31051725, -1.31048123, -1.31101296, -1.31023039]),\n",
       "  'split3_test_score': array([-1.2603335 , -1.26032785, -1.26033712, -1.26080918, -1.26070494]),\n",
       "  'split4_test_score': array([-0.99374876, -0.99364768, -0.99364139, -0.99509237, -0.99362422]),\n",
       "  'mean_test_score': array([-1.22308   , -1.2231523 , -1.22321298, -1.2236102 , -1.22011355]),\n",
       "  'std_test_score': array([0.14614403, 0.14626295, 0.14631682, 0.1458132 , 0.14237837]),\n",
       "  'rank_test_score': array([2, 3, 4, 5, 1], dtype=int32)},\n",
       " {'gamma': 0.4},\n",
       " -1.2201135516075285)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "gsearch3 = RandomizedSearchCV(\n",
    "    estimator = xgb_model.get_multi_class_classifier_model(n_estimators=109,max_depth=2,min_child_weight=0),\n",
    "    param_distributions = param_test3,\n",
    "    scoring='neg_log_loss',\n",
    "    n_jobs=4,iid=False, \n",
    "    cv=5)\n",
    "gsearch3.fit(X_train_verlander,Y_train_verlander)\n",
    "gsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a gamma value of 0.4 had the best result. Before proceeding, a good idea would be to re-calibrate the number of boosting rounds for the updated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy (Train) : 0.6265900310939414\n",
      "Accuracy (Test) : 0.5359336609336609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'gamma': 0.4,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 2,\n",
       " 'min_child_weight': 0,\n",
       " 'missing': None,\n",
       " 'n_estimators': 204,\n",
       " 'n_jobs': 1,\n",
       " 'nthread': 4,\n",
       " 'objective': 'multi:softprob',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': None,\n",
       " 'silent': None,\n",
       " 'subsample': 0.8,\n",
       " 'verbosity': 1,\n",
       " 'eval_metric': 'mlogloss',\n",
       " 'num_class': 16}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_verlander=xgb_model.get_multi_class_classifier_model(n_estimators=1000,max_depth=2,min_child_weight=0,gamma=0.4)\n",
    "xgb_model.fit_multi_class_model(model=model_verlander,x_train=X_train_verlander,y_train=Y_train_verlander,x_test=X_test_verlander,y_test=Y_test_verlander,useTrainCV=True)\n",
    "model_verlander.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimum number of estimators is 204\n"
     ]
    }
   ],
   "source": [
    "n_estimators= model_verlander.n_estimators\n",
    "print(\"The optimum number of estimators is {}\".format(n_estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Tune subsample and colsample_by_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([14.12881689, 14.19526815, 16.98243375, 18.4165936 , 20.063799  ,\n",
       "         17.61839771, 15.63951597, 17.37577143, 17.31912136, 11.24061341]),\n",
       "  'std_fit_time': array([0.63018458, 0.80141734, 1.42638164, 0.92653159, 1.03417687,\n",
       "         1.20366412, 1.27834258, 0.73296874, 1.01065651, 4.7304777 ]),\n",
       "  'mean_score_time': array([0.0478672 , 0.05511346, 0.05680413, 0.05746722, 0.05461531,\n",
       "         0.05404429, 0.05525284, 0.05612359, 0.05515079, 0.04082279]),\n",
       "  'std_score_time': array([0.00910149, 0.00590349, 0.00511344, 0.02143465, 0.00838632,\n",
       "         0.0077109 , 0.00535329, 0.00333429, 0.0067736 , 0.01578441]),\n",
       "  'param_subsample': masked_array(data=[0.7, 0.8, 0.6, 0.7, 0.7, 0.6, 0.8, 0.8, 0.8, 0.9],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_colsample_bytree': masked_array(data=[0.6, 0.7, 0.8, 0.8, 0.9, 0.7, 0.6, 0.9, 0.8, 0.7],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'subsample': 0.7, 'colsample_bytree': 0.6},\n",
       "   {'subsample': 0.8, 'colsample_bytree': 0.7},\n",
       "   {'subsample': 0.6, 'colsample_bytree': 0.8},\n",
       "   {'subsample': 0.7, 'colsample_bytree': 0.8},\n",
       "   {'subsample': 0.7, 'colsample_bytree': 0.9},\n",
       "   {'subsample': 0.6, 'colsample_bytree': 0.7},\n",
       "   {'subsample': 0.8, 'colsample_bytree': 0.6},\n",
       "   {'subsample': 0.8, 'colsample_bytree': 0.9},\n",
       "   {'subsample': 0.8, 'colsample_bytree': 0.8},\n",
       "   {'subsample': 0.9, 'colsample_bytree': 0.7}],\n",
       "  'split0_test_score': array([-1.21921981, -1.22247951, -1.22433155, -1.21927002, -1.23229381,\n",
       "         -1.23367555, -1.22084614, -1.21430973, -1.21357721, -1.22143175]),\n",
       "  'split1_test_score': array([-1.55732276, -1.61517806, -1.65396262, -1.59245614, -1.5870933 ,\n",
       "         -1.62244525, -1.55054319, -1.64483076, -1.62422247, -1.6309903 ]),\n",
       "  'split2_test_score': array([-1.39796462, -1.4113361 , -1.38364751, -1.40752711, -1.3981552 ,\n",
       "         -1.38010833, -1.42147872, -1.42825863, -1.43056987, -1.41723669]),\n",
       "  'split3_test_score': array([-1.33032545, -1.36752771, -1.32490627, -1.3640351 , -1.36823045,\n",
       "         -1.31593001, -1.37230971, -1.39275325, -1.39608353, -1.38248489]),\n",
       "  'split4_test_score': array([-1.00477873, -1.00604819, -1.00482247, -1.00672042, -1.00784282,\n",
       "         -1.00249436, -1.00707398, -1.00972345, -1.00621277, -1.0085994 ]),\n",
       "  'mean_test_score': array([-1.30192227, -1.32451391, -1.31833408, -1.31800176, -1.31872312,\n",
       "         -1.3109307 , -1.31445035, -1.33797516, -1.33413317, -1.33214861]),\n",
       "  'std_test_score': array([0.18460458, 0.20283262, 0.21169863, 0.19601178, 0.19230162,\n",
       "         0.20147118, 0.18643207, 0.21370747, 0.20945416, 0.20785687]),\n",
       "  'rank_test_score': array([ 1,  7,  5,  4,  6,  2,  3, 10,  9,  8], dtype=int32)},\n",
       " {'subsample': 0.7, 'colsample_bytree': 0.6},\n",
       " -1.3019222724235582)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "gsearch4 = RandomizedSearchCV(\n",
    "    estimator = xgb_model.get_multi_class_classifier_model(n_estimators=204,max_depth=2,min_child_weight=0,gamma=0.4),\n",
    "    param_distributions = param_test4,\n",
    "    scoring='neg_log_loss',\n",
    "    n_jobs=4,iid=False, \n",
    "    cv=5)\n",
    "\n",
    "gsearch4.fit(X_train_verlander,Y_train_verlander)\n",
    "gsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best subsample by .7, colsample was .6\n",
    "#### Step 5: Tuning Regularization Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([14.32710137, 14.57248316, 17.17778926, 16.13747025, 17.37447286,\n",
       "         14.85060148, 15.67457061, 18.30908766, 17.83774796, 13.96061387]),\n",
       "  'std_fit_time': array([1.08210311, 0.66788265, 1.38279229, 0.56434194, 1.38882582,\n",
       "         0.75564499, 0.91837722, 0.73157641, 1.70784412, 3.16162234]),\n",
       "  'mean_score_time': array([0.03454432, 0.05101638, 0.04878716, 0.04872494, 0.05497708,\n",
       "         0.03457971, 0.04582353, 0.05421638, 0.05220637, 0.04351907]),\n",
       "  'std_score_time': array([0.00586807, 0.00768105, 0.00489329, 0.0090831 , 0.00336037,\n",
       "         0.00380441, 0.00791832, 0.00212754, 0.00847935, 0.00977824]),\n",
       "  'param_reg_lambda': masked_array(data=[0.1, 1, 0.01, 1e-05, 0.01, 1e-05, 100, 1e-05, 0.1, 0.1],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_reg_alpha': masked_array(data=[100, 100, 1, 1, 0.1, 100, 100, 0.1, 0.1, 1e-05],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'reg_lambda': 0.1, 'reg_alpha': 100},\n",
       "   {'reg_lambda': 1, 'reg_alpha': 100},\n",
       "   {'reg_lambda': 0.01, 'reg_alpha': 1},\n",
       "   {'reg_lambda': 1e-05, 'reg_alpha': 1},\n",
       "   {'reg_lambda': 0.01, 'reg_alpha': 0.1},\n",
       "   {'reg_lambda': 1e-05, 'reg_alpha': 100},\n",
       "   {'reg_lambda': 100, 'reg_alpha': 100},\n",
       "   {'reg_lambda': 1e-05, 'reg_alpha': 0.1},\n",
       "   {'reg_lambda': 0.1, 'reg_alpha': 0.1},\n",
       "   {'reg_lambda': 0.1, 'reg_alpha': 1e-05}],\n",
       "  'split0_test_score': array([-1.09464646, -1.09465886, -1.23765845, -1.23377342, -1.2212708 ,\n",
       "         -1.0946451 , -1.09511474, -1.23476693, -1.23636437, -1.22979534]),\n",
       "  'split1_test_score': array([-1.07824695, -1.0785924 , -1.62559622, -1.62591928, -1.62185877,\n",
       "         -1.07824364, -1.07865641, -1.62127656, -1.63722582, -1.60928965]),\n",
       "  'split2_test_score': array([-1.12922118, -1.12941405, -1.42469528, -1.42625469, -1.4215574 ,\n",
       "         -1.12922246, -1.12829257, -1.42494255, -1.42704869, -1.41793833]),\n",
       "  'split3_test_score': array([-1.107964  , -1.10795445, -1.32472691, -1.32400875, -1.34955992,\n",
       "         -1.10796338, -1.10942062, -1.36973487, -1.36791344, -1.36354158]),\n",
       "  'split4_test_score': array([-1.04087384, -1.0408979 , -1.00560804, -1.00534424, -1.02322645,\n",
       "         -1.04087113, -1.04368828, -1.02854199, -1.01218015, -1.01891499]),\n",
       "  'mean_test_score': array([-1.09019049, -1.09030353, -1.32365698, -1.32306007, -1.32749467,\n",
       "         -1.09018914, -1.09103452, -1.33585258, -1.33614649, -1.32789598]),\n",
       "  'std_test_score': array([0.02977895, 0.02979337, 0.20490391, 0.20556711, 0.19992376,\n",
       "         0.02978033, 0.02876715, 0.19759048, 0.20728327, 0.19680123]),\n",
       "  'rank_test_score': array([ 2,  3,  6,  5,  7,  1,  4,  9, 10,  8], dtype=int32)},\n",
       " {'reg_lambda': 1e-05, 'reg_alpha': 100},\n",
       " -1.0901891417311527)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n",
    " 'reg_lambda':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "gsearch5 = RandomizedSearchCV(\n",
    "    estimator = xgb_model.get_multi_class_classifier_model(n_estimators=204,max_depth=2,min_child_weight=0,gamma=0.4,subsample=.7,colsample_bytree=0.8),\n",
    "    param_distributions = param_test5,\n",
    "    scoring='neg_log_loss',\n",
    "    n_jobs=4,iid=False, \n",
    "    cv=5)\n",
    "\n",
    "gsearch5.fit(X_train_verlander,Y_train_verlander)\n",
    "gsearch5.cv_results_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimum value for reg_alpha = 100. reg_lambda=1e-5. \n",
    "\n",
    "#### Step 6: Reduce learning rate\n",
    "Lastly, we should lower the learning rate and add more trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model_verlander=XGBClassifier(learning_rate=.05,n_estimators=2000,max_depth=2,min_child_weight=0,gamma=0.4,subsample=.7,colsample_bytree=0.8,reg_lambda=1e-5,reg_alpha=100,metric='mlogloss',seed=1301,num_threads=4,objective='multi:softprob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy (Train) : 0.6031282389522284\n",
      "Accuracy (Test) : 0.5153562653562653\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit_multi_class_model(model=model_verlander,x_train=X_train_verlander,y_train=Y_train_verlander,x_test=X_test_verlander,y_test=Y_test_verlander,useTrainCV=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimum number of estimators is 1000\n"
     ]
    }
   ],
   "source": [
    "n_estimators= model_verlander.n_estimators\n",
    "print(\"The optimum number of estimators is {}\".format(n_estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for other pitchers\n",
    "Now that we've tuned our xgboost model, we'll use it to train a model for other pitchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the NN data for Scherzer\n",
    "X_test_scherzer,Y_test_scherzer = get_X_Y(pd_test_scherzer,num_pitch_types)\n",
    "X_train_scherzer,Y_train_scherzer = get_X_Y(pd_train_scherzer,num_pitch_types)\n",
    "\n",
    "# get the NN data for Porcello\n",
    "X_test_porcello,Y_test_porcello = get_X_Y(pd_test_porcello,num_pitch_types)\n",
    "X_train_porcello,Y_train_porcello = get_X_Y(pd_train_porcello,num_pitch_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scherzer=XGBClassifier(learning_rate=.05,n_estimators=2000,max_depth=2,min_child_weight=0,gamma=0.4,subsample=.7,colsample_bytree=0.8,reg_lambda=1e-5,reg_alpha=100,metric='mlogloss',seed=1301,num_threads=4,objective='multi:softprob')\n",
    "model_porcello=XGBClassifier(learning_rate=.05,n_estimators=2000,max_depth=2,min_child_weight=0,gamma=0.4,subsample=.7,colsample_bytree=0.8,reg_lambda=1e-5,reg_alpha=100,metric='mlogloss',seed=1301,num_threads=4,objective='multi:softprob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy (Train) : 0.5122216724393864\n",
      "Accuracy (Test) : 0.48663906661648476\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit_multi_class_model(model=model_scherzer,x_train=X_train_scherzer,y_train=Y_train_scherzer,x_test=X_test_scherzer,y_test=Y_test_scherzer,useTrainCV=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/paperspace/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy (Train) : 0.40432288465478383\n",
      "Accuracy (Test) : 0.3956081081081081\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit_multi_class_model(model=model_porcello,x_train=X_train_porcello,y_train=Y_train_porcello,x_test=X_test_porcello,y_test=Y_test_porcello,useTrainCV=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_verlander = model_verlander.predict(X_test_verlander)\n",
    "y_pred_scherzer = model_scherzer.predict(X_test_scherzer)\n",
    "y_pred_porcello = model_porcello.predict(X_test_porcello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_verlander = confusion_matrix(Y_test_verlander,y_pred_verlander)\n",
    "cfm_scherzer = confusion_matrix(Y_test_scherzer,y_pred_scherzer)\n",
    "cfm_porcello = confusion_matrix(Y_test_porcello,y_pred_porcello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1598,    0,   28,    0,    1],\n",
       "       [   1,    0,    0,    0,    0],\n",
       "       [ 851,    0,   80,    0,    0],\n",
       "       [ 131,    0,    5,    0,    0],\n",
       "       [ 540,    0,   21,    0,    0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
